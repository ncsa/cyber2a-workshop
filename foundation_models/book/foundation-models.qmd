---
title: 'Foundation Models: The Cornerstones of Modern AI'
---


## Instructors

* Sandeep Puthanveetil Satheesan <sandeeps@illinois.edu>, Sr. Research Software Engineer National Center for Supercomputing Applications (NCSA) University of Illinois Urbana-Champaign 
* Minu Mathew <minum@illinois.edu>, Research Software Engineer National Center for Supercomputing Applications (NCSA) University of Illinois Urbana-Champaign 

## Overview

Foundation models (FM) are deep learning models trained on massive raw unlabelled datasets usually through self-supervised learning. FMs enable todayâ€™s data scientists to use them as the base and fine-tune using domain specific data to obtain models that can handle a wide range of tasks [1, 6, 7]. In this talk, we provide an introduction to FMs, its history, evolution, and go through its key features and categories, and a few examples. We also briefly discuss how foundation models work. This talk will be a precursor to the hands-on session that follows on the same topic.

::: {.column-margin}
![](images/foundation_models.png)
:::

Image source: 2021 paper on foundation models by Stanford researchers [@Bommasani2021FoundationModels;].

In this session, we take a closer look at what constitutes a foundation model, a few examples, and some basic principles around how it works.

## Outline 
1. Overview of foundation models
2. Types of foundation models
3. Architecture
3. Segment Anything Model (SAM 2)
4. Retrieval Augmented Generation


## Introduction

### Traditional ML vs Deep Learning vs Foundation Models

**Traditional machine learning** involves algorithms that learn patterns from structured data. Techniques like decision trees, support vector machines, and linear regression fall under this category. These methods often require feature engineering, where domain knowledge is used to select and transform input features to improve model performance. Traditional machine learning excels in scenarios with limited data and interpretable results.

**Deep learning** is a subset of machine learning that employs neural networks with multiple layers (hence "deep"). These models automatically learn features from raw data, making them particularly powerful for complex tasks like image and speech recognition. Deep learning excels with large datasets and can capture intricate patterns but often requires significant computational resources and can be harder to interpret compared to traditional methods.

**Foundation models**, such as GPT and BERT, represent a new paradigm in AI. These large-scale models are pre-trained on vast amounts of data and can be fine-tuned for specific tasks with minimal additional training. They are designed to understand and generate human-like text, making them versatile for various applications like natural language processing and computer vision. Foundation models leverage transfer learning, allowing them to generalize across different tasks more effectively than traditional machine learning and deep learning models.



## Foundation Models

Foundation models, introduced in 2021 by Standford Researchers [@Bommasani2021FoundationModels;], are characterized by their enormous neural networks trained on vast datasets through self-supervised learning. Their intelligence improves with more data, requiring substantial computational power for training. These models can be adapted to various downstream tasks and are designed for reuse, leveraging transfer learning to enhance performance across different applications.
::: {.column-margin}
![](images/foundation_models_paper.png)
:::

