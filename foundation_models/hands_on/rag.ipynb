{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Hands-on Tutorial\n",
    "\n",
    "Will be using [Langchain framework](https://www.langchain.com/)\n",
    "\n",
    "Suggested code references:\n",
    "- Langchain RAG from scratch [link](https://github.com/langchain-ai/rag-from-scratch/tree/main)\n",
    "- Langchain RAG quickstart [link](https://python.langchain.com/v0.1/docs/use_cases/question_answering/quickstart/)\n",
    "\n",
    "Session hands-on code in [github.com/ncsa/cyber2a-workshop](github.com/ncsa/cyber2a-workshop)\n",
    "\n",
    "Session technical details in course book : [cyber2a.github.io/cyber2a-course/sections/foundation-models.html](https://cyber2a.github.io/cyber2a-course/sections/foundation-models.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "![With and Without RAG](notebook_images/rag-with-without.png \"With and Without RAG\") \n",
    "\n",
    "![RAG](notebook_images/rag-before-after.png \"RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG - *Retrieval*-Augmented Generation\n",
    "\n",
    "### Knowledge DB\n",
    "\n",
    "* Vector database (Beginners [blog 1](https://medium.com/data-and-beyond/vector-databases-a-beginners-guide-b050cbbe9ca0), Pinecone [blog 2](https://www.pinecone.io/learn/vector-database/))\n",
    "\n",
    "\n",
    "![knowledge DB](notebook_images/knowledge-db.png \"Knowledge DB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG - *Retrieval*-Augmented Generation\n",
    "\n",
    "### Vector DB Retrieval\n",
    "\n",
    "![Vector DB Retrieval](notebook_images/vectordb-retrieval.png \"Vector DB Retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG - Retrieval Steps\n",
    "\n",
    "1. Prepare data \n",
    "2. Create a vector store and insert data\n",
    "3. Search the vector store and retrieve relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# create and configure logger\n",
    "logging.basicConfig(level=logging.INFO, datefmt='%Y-%m-%dT%H:%M:%S',\n",
    "                    format='%(asctime)-15s.%(msecs)03dZ %(levelname)-7s : %(name)s - %(message)s',\n",
    "                    handlers=[logging.StreamHandler(sys.stdout)]\n",
    "                    )\n",
    "# create log object with current module name\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare data\n",
    "- Load data from different sources\n",
    "- Will be using proceedings from [Arctic data symposium 2023](https://arcticdata.io/catalog/portals/pisymposium2023).\n",
    "- Eg: [Proceedings final report](https://permafrostcoasts.org/wp-content/uploads/2024/08/2023-PI-Symposium-final-report-web.pdf), [participant bios](https://arcticdata.io/metacat/d1/mn/v2/object/urn:uuid:6e613b84-842c-4ac9-993d-a863d7040aa5) \n",
    "- Data in data/docs directory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Loaders\n",
    "- Langchain provides different data loaders for different file types\n",
    "- Data loaded in Langchain Document class format [document class](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
    "\n",
    "![Langchain document class](notebook_images/langchain-document-class.png \"Langchain document class\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "from langchain_community.document_loaders import CSVLoader, DataFrameLoader, PyPDFLoader, Docx2txtLoader, UnstructuredRSTLoader, DirectoryLoader\n",
    "\n",
    "\n",
    "class DataLoaders:\n",
    "    \"\"\"\n",
    "    various data loaders\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir_path):\n",
    "        self.data_dir_path = data_dir_path\n",
    "    \n",
    "    def csv_loader(self):\n",
    "        csv_loader_kwargs = {\n",
    "                            \"csv_args\":{\n",
    "                                \"delimiter\": \",\",\n",
    "                                \"quotechar\": '\"',\n",
    "                                },\n",
    "                            }\n",
    "        dir_csv_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.csv\", use_multithreading=True,\n",
    "                                    loader_cls=CSVLoader, \n",
    "                                    loader_kwargs=csv_loader_kwargs,\n",
    "                                    )\n",
    "        return dir_csv_loader\n",
    "    \n",
    "    def pdf_loader(self):\n",
    "        dir_pdf_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.pdf\",\n",
    "                                    loader_cls=PyPDFLoader,\n",
    "                                    )\n",
    "        return dir_pdf_loader\n",
    "    \n",
    "    def word_loader(self):\n",
    "        dir_word_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.docx\",\n",
    "                                    loader_cls=Docx2txtLoader,\n",
    "                                    )\n",
    "        return dir_word_loader\n",
    "    \n",
    "    def rst_loader(self):\n",
    "        rst_loader_kwargs = {\n",
    "                        \"mode\":\"single\"\n",
    "                        }\n",
    "        dir_rst_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.rst\",\n",
    "                                    loader_cls=UnstructuredRSTLoader, \n",
    "                                    loader_kwargs=rst_loader_kwargs\n",
    "                                    )\n",
    "        return dir_rst_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_dir_path = os.getenv('DATA_DIR_PATH', \"data\")\n",
    "data_loader = DataLoaders(data_dir_path=data_dir_path)\n",
    "log.info(\"Loading files from directory %s\", data_dir_path)\n",
    "dir_csv_loader = data_loader.csv_loader()\n",
    "dir_word_loader = data_loader.word_loader()\n",
    "dir_pdf_loader = data_loader.pdf_loader()\n",
    "dir_rst_loader = data_loader.rst_loader()\n",
    "csv_data = dir_csv_loader.load()\n",
    "word_data = dir_word_loader.load()\n",
    "pdf_data = dir_pdf_loader.load()\n",
    "rst_data = dir_rst_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in pdf_data:\n",
    "    print(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Format into text and metadata\n",
    "- Convert data to a list of texts and metadata \n",
    "- Metadata can be used for filtering the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text and metadata from the data\n",
    "def get_text_metadatas(csv_data=None, pdf_data=None, word_data=None, rst_data=None):\n",
    "    \"\"\"\n",
    "    Each document class has page_content and metadata properties\n",
    "    Separate text and metadata content from Document class\n",
    "    Have custom metadata if needed\n",
    "    \"\"\"\n",
    "    csv_texts = [doc.page_content for doc in csv_data]\n",
    "    # custom metadata\n",
    "    csv_metadatas = [{'source': doc.metadata['source'], 'row_page': doc.metadata['row']} for doc in csv_data]   # metadata={'source': 'filename.csv', 'row': 0}\n",
    "    pdf_texts = [doc.page_content for doc in pdf_data]\n",
    "    pdf_metadatas = [{'source': doc.metadata['source'], 'row_page': doc.metadata['page']} for doc in pdf_data]  # metadata={'source': 'data/filename.pdf', 'page': 8}\n",
    "    word_texts = [doc.page_content for doc in word_data]\n",
    "    word_metadatas = [{'source': doc.metadata['source'], 'row_page': ''} for doc in word_data] \n",
    "    rst_texts = [doc.page_content for doc in rst_data]\n",
    "    rst_metadatas = [{'source': doc.metadata['source'], 'row_page': ''} for doc in rst_data]         # metadata={'source': 'docs/images/architecture/index.rst'}\n",
    "\n",
    "    texts = csv_texts + pdf_texts + word_texts + rst_texts\n",
    "    metadatas = csv_metadatas + pdf_metadatas + word_metadatas + rst_metadatas\n",
    "    return texts, metadatas\n",
    "\n",
    "\n",
    "texts , metadatas = get_text_metadatas(csv_data, pdf_data, word_data, rst_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Chunking\n",
    "\n",
    "![Chunk Optimization](notebook_images/rag-chunking.png \"Chunk Optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Chunking\n",
    "- Split texts into chunks for embedding\n",
    "- Return a list of document chunks (list of langchain [document class](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\n",
    "            \"\\n\\n\", \"\\n\", \". \", \" \", \"\"\n",
    "        ]  # try to split on paragraphs... fallback to sentences, then chars, ensure we always fit in context window\n",
    "    )\n",
    "\n",
    "docs: List[Document] = text_splitter.create_documents(texts=texts, metadatas=metadatas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0])\n",
    "print(\"Number of documents: \", len(docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Embeddings\n",
    "\n",
    "- Mathematical representations of data points in a high-dimensional space. \n",
    "- In the context of natural language processing:\n",
    "    1. Word Embeddings: Individual words are represented as real-valued vectors in a multi-dimensional space.\n",
    "    2. Semantic Capture: These embeddings capture the semantic meaning and relationships of the text.\n",
    "    3. Similarity Principle: Words with similar meanings tend to have similar vector representations.\n",
    "\n",
    "- We will be using OpenAI embeddings\n",
    "- text-embedding-ada-002 model for embeddings, which has a maximum token limit of 8191 according to OpenAI documentation.\n",
    "- HF Embedding models leaderboard [link](https://huggingface.co/spaces/mteb/leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG - Retrieval Steps\n",
    "\n",
    "~~1. Prepare data ~~\n",
    "2. Create a vector store and insert data\n",
    "3. Search the vector store and retrieve relevant documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vector Store\n",
    "\n",
    "![Inserting into DB](notebook_images/inserting-db.png \"Inserting into DB\")\n",
    "\n",
    "Source Credits : [Blog.demir](https://blog.demir.io/hands-on-with-rag-step-by-step-guide-to-integrating-retrieval-augmented-generation-in-llms-ac3cb075ab6f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vector Store\n",
    "\n",
    "- We will use [Qdrant](https://qdrant.tech/) vector store for this example\n",
    "- For today we will use local memory as the vector store\n",
    "- Qdrant has a docker image that can be used to create a vector store and hosted remotely\n",
    "Eg: [Qdrant docker container running locally](http://localhost:6333/dashboard)\n",
    "\n",
    "- Blog post on vector stores [link](https://medium.com/google-cloud/vector-databases-are-all-the-rage-872c888fa348)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a qdrant vector store in local memory\n",
    "\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "# qdrant collection name\n",
    "collection_name = os.getenv('QDRANT_COLLECTION_NAME', \"data-collection\")\n",
    "\n",
    "# create vector store in local memory\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=collection_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG - Retrieval Steps\n",
    "\n",
    "~~1. Prepare data ~~\n",
    "~~2. Create a vector store and insert data~~\n",
    "3. Search the vector store and retrieve relevant documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve relevant documents\n",
    "Create a retriever from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever to retrieve relevant snippets\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG - Retrieval Steps\n",
    "\n",
    "~~1. Prepare data ~~\n",
    "~~2. Create a vector store and insert data~~\n",
    "~~3. Search the vector store and retrieve relevant documents~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG - Retrieval-Augmented *Generation*\n",
    "\n",
    "\n",
    "![RAG LLM](notebook_images/rag-llm.png \"RAG LLM\")\n",
    "\n",
    "![LLM prompting](notebook_images/rag-prompting.png \"LLM Prompting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Call LLM\n",
    "\n",
    "### 4.1 Prompting\n",
    "- Use a prompt template [link](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html)\n",
    "    - includes input parameters that can be dynamically changed\n",
    "- Use Langchain hub to pull prompts [link](https://smith.langchain.com/hub)\n",
    "    - easy to share and reuse prompts\n",
    "    - can see what are the popular prompts for specific use cases\n",
    "    - Eg: [rag-prompt](https://smith.langchain.com/hub/rlm/rag-prompt)\n",
    "- Use a custom prompt\n",
    "```\n",
    "qa_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "    1. If the question has some initial findings, use that as context.\n",
    "    2. If you don't know the answer, don't try to make up an answer. Just say **I can't find the final answer but you may want to check the following sourcess** and add the source documents as a list.\n",
    "    3. If you find the answer, write the answer in a concise way and add the list of sources that are **directly** used to derive the answer. Exclude the sources that are irrelevant to the final answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "rag_chain_prompt = PromptTemplate.from_template(qa_prompt_template) \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Prompting\n",
    "\n",
    "rlm/rag-prompt from Langchain\n",
    "\n",
    "![RLM RAG prompt](notebook_images/rlm-rag-prompt.png \"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompting\n",
    "\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Call LLM\n",
    "- We will use \n",
    "    - OpenAI GPT-4o-mini and \n",
    "    - Ollama llama3.2 model (hosted by NCSA)\n",
    "- Each model has its own formats and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting the documents as a string before calling the LLM\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call open ai GPT-4o-mini\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# create a chat openai model\n",
    "llm: ChatOpenAI = ChatOpenAI(\n",
    "            temperature=0,\n",
    "            model=\"gpt-4o-mini\",\n",
    "            max_retries=500,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call GPT4o-mini\n",
    "llm.invoke(\"What is the capital of the world?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 RAG \n",
    "\n",
    "![RAG system](notebook_images/rag-system.png \"RAG system\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 RAG Chain\n",
    "Combining it all together\n",
    "\n",
    "- Context is the retrieved docs from the retriever/vector db\n",
    "- RunnablePassthrough() is used to pass the user query as is to the chain\n",
    "- format_docs is used to format the documents as a string\n",
    "- prompt is used to call the prompt template\n",
    "- llm is used to call the LLM\n",
    "- StrOutputParser() is used to parse the output from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "openai_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call openai rag chain\n",
    "openai_rag_chain.invoke(\"What were the goals of the symposium?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call openai rag chain\n",
    "openai_rag_chain.invoke(\"What is the capital of the world?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call ollama llama3:latest\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "ollama_jwt_token = os.getenv('OLLAMA_JWT_TOKEN')\n",
    "ollama_headers = {\"Authorization\": f\"Bearer {ollama_api_key}\"}\n",
    "\n",
    "# create a ollama model\n",
    "ollamallm: Ollama = Ollama(\n",
    "    base_url=\"https://ollama.software.ncsa.illinois.edu/ollama\",\n",
    "    model=\"llama3.2:latest\",\n",
    "    headers=ollama_headers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call llama3 model\n",
    "ollamallm.invoke(\"What is the capital of the world?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama rag chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "ollama_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ollamallm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call ollama rag chain\n",
    "ollama_rag_chain.invoke(\"Who is the president of USA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding sources to openai rag chain\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "openai_rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "openai_rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=openai_rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call openai rag chain with source\n",
    "# this will return the answer and the sources (context)\n",
    "openai_rag_chain_with_source.invoke(\"What were the goals of the symposium?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_rag_chain_with_source.invoke(\"Why is tundra restoration and rehabilitation important\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_rag_chain_with_source.invoke(\"Who is Brenadette Adams?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Steps\n",
    "\n",
    "1. Prepare data \n",
    "2. Create a vector store and insert into db\n",
    "3. Search the vector store and retrieve relevant documents\n",
    "4. Call LLM with the user query and the retrieved documents\n",
    "4. Return the LLM response to the user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
