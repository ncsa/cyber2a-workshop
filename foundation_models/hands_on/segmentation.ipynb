{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cyber2A Workshop Hands-on Lab: Foundation Models - Image Segmentation using Segment Anything Model 2 (SAM 2)",
   "id": "8f20cc4914937e6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This notebook reuses some code segments (e.g., helper methods, imports, loading the model, etc.) from the [image predictor example](https://github.com/facebookresearch/sam2/blob/main/notebooks/image_predictor_example.ipynb) initially published in the SAM 2 source code repository.\n",
    "SAM 2 source code is released under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0), and <i>Meta Platforms, Inc. and affiliates</i> hold the copyright for the [image_predictor_example.ipynb](https://github.com/facebookresearch/sam2/blob/main/notebooks/image_predictor_example.ipynb) notebook.\n",
    "We have adapted and modified the image predictor example notebook to use data files from Arctic datasets and included specific activities for the Cyber2A Workshop.\n",
    "</div>"
   ],
   "id": "75eaf9441b57306e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "SAM-2 is a Promptable Visual Segmentation (PVS) model trained on large-scale generic data that can predict object segmentation masks based on input prompts. These prompts can be a point, bounding box (i.e., a rectangle), mask, or a combination.\n",
    "\n",
    "The model converts the image into an image embedding (a dense vector representation of the image), which is used to predict masks based on a user prompt.\n",
    "\n",
    "The `SAM2ImagePredictor` class provides an easy interface to the model. Users can attach the input image to the model using its `set_image` method, which calculates the image embeddings. Then, users can use the `predict` method to share prompts (user inputs) that help with the segmentation mask prediction."
   ],
   "id": "b4a4b25c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set-up",
   "id": "0be845da"
  },
  {
   "cell_type": "markdown",
   "id": "33681dd1",
   "metadata": {
    "id": "33681dd1"
   },
   "source": "Necessary package and checkpoints' imports and helper functions for displaying points, boxes, and masks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b28288",
   "metadata": {
    "id": "69b28288"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741223218cc54186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SAM-2 checkpoints (saved versions of the model along with its parameters).\n",
    "# For the Cyber2A hands-on session, we have already downloaded the checkpoints. Hence, we have commented out the download code below. If you are running this code later, you will need to uncomment the code below to download the checkpoints.\n",
    "\n",
    "# os.chdir(\"SAM_checkpoints\")\n",
    "# !sh download_checkpoints.sh\n",
    "# os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a15e2f-c7e1-4e5d-862f-fcb751a60b89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33a15e2f-c7e1-4e5d-862f-fcb751a60b89",
    "outputId": "b2779686-e178-4076-8c8c-aed237e51b64"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# Select the device for computation. We will be using CUDA to run this notebook. Other options are provided for running this notebook in different environments.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc90d5",
   "metadata": {
    "id": "29bc90d5"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "np.random.seed(3)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23842fb2",
   "metadata": {
    "id": "23842fb2"
   },
   "source": [
    "## Example image 1\n",
    "\n",
    "Open the first example image, create an object, and display it with grid for estimating point and box coordinates.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e4f6b",
   "metadata": {
    "id": "3c2e4f6b"
   },
   "outputs": [],
   "source": [
    "image = Image.open('data/images/20180917-112527-reduced.jpg')\n",
    "image = np.array(image.convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30125fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "e30125fd",
    "outputId": "ebc2bf0c-9839-446b-8f84-56d19e5fa66e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.grid(visible=True)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b228b8",
   "metadata": {
    "id": "98b228b8"
   },
   "source": "## Loading the SAM 2 model and configuration"
  },
  {
   "cell_type": "markdown",
   "id": "0bb1927b",
   "metadata": {
    "id": "0bb1927b"
   },
   "source": "Load the SAM 2 model and predictor. Here, we provide the path to a SAM 2 checkpoint, and it's corresponding configuration YAML file (added during SAM 2 installation)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28150b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e28150b",
    "outputId": "0d9f0408-f222-49a5-dee0-455a8feb9419"
   },
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "sam2_checkpoint = \"SAM_checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c925e829",
   "metadata": {
    "id": "c925e829"
   },
   "source": "Process the image to produce an image embedding by calling `SAM2ImagePredictor.set_image`. `SAM2ImagePredictor` stores this embedding and will use it for subsequent mask prediction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d48dd",
   "metadata": {
    "id": "d95d48dd"
   },
   "outputs": [],
   "source": [
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc7a46",
   "metadata": {
    "id": "d8fc7a46"
   },
   "source": [
    "### Specifying an object or region using a single point\n",
    "\n",
    "In this example image, to prompt for the glacier region, let's choose a point on it. \n",
    "\n",
    "Points are a type of input to the model. It's represented in (x,y) format and comes with corresponding labels 1 or 0, which are used to represent foreground and background respectively. As we will see later, we can use multiple points as input, but here we use only one. The show_points method displays the selected point using a star icon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69570c",
   "metadata": {
    "id": "5c69570c"
   },
   "outputs": [],
   "source": [
    "input_point = np.array([[600, 400]])\n",
    "input_label = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ba973",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "a91ba973",
    "outputId": "539ccb56-0f84-438c-b34d-658b643673af"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073a838",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1073a838",
    "outputId": "be3c2f7d-281c-44fb-a18b-5d8bc7c612bf"
   },
   "outputs": [],
   "source": [
    "# Display the image embedding feature dimension\n",
    "print(predictor._features[\"image_embed\"].shape, predictor._features[\"image_embed\"][-1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765e952",
   "metadata": {
    "id": "c765e952"
   },
   "source": "Predict segmentation mask with `SAM2ImagePredictor.predict`. The model returns segmentation masks, quality predictions for those masks, and low resolution mask logits that can be passed to the next iteration of prediction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373fd68",
   "metadata": {
    "id": "5373fd68"
   },
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True,\n",
    ")\n",
    "sorted_ind = np.argsort(scores)[::-1] # Sorting the scores in decreasing order\n",
    "masks = masks[sorted_ind]\n",
    "scores = scores[sorted_ind]\n",
    "logits = logits[sorted_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0e938",
   "metadata": {
    "id": "c7f0e938"
   },
   "source": [
    "With `multimask_output=True` (the default setting), SAM 2 outputs 3 masks, where `scores` gives the model's own estimation of the quality of these masks. This setting is intended for ambiguous input prompts, and helps the model disambiguate different objects consistent with the prompt. When `False`, it will return a single mask. For ambiguous prompts such as a single point, it is recommended to use `multimask_output=True` even if only a single mask is desired; the best single mask can be chosen by picking the one with the highest score returned in `scores`. This will often result in a better mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47821187",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47821187",
    "outputId": "8e203298-c55a-4a0a-cfb1-13a04f61eae7"
   },
   "outputs": [],
   "source": [
    "masks.shape  # (number_of_masks) x H x W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c227a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e9c227a6",
    "outputId": "128ad5c6-35fa-4bd3-a3b5-9489677eaf88"
   },
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label, borders=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa31f7c",
   "metadata": {
    "id": "3fa31f7c"
   },
   "source": "## Activity 1: Specifying an object or region using multiple points"
  },
  {
   "cell_type": "markdown",
   "id": "88d6d29a",
   "metadata": {
    "id": "88d6d29a"
   },
   "source": "We can see that the single input point can be ambiguous, and the model has returned multiple sub-regions within the glacier image. To obtain a single object or region without ambiguity, multiple points can be provided. If available, a mask from a previous iteration can also be supplied to the model to aid in prediction. When specifying a single object with multiple prompts, a single mask can be requested by setting `multimask_output=False`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6923b94",
   "metadata": {
    "id": "f6923b94"
   },
   "outputs": [],
   "source": [
    "# E.g., input format for specifying two points\n",
    "# input_point = np.array([[x1, y1], [x2, y2]])\n",
    "# input_label = np.array([1, 1])\n",
    "\n",
    "# TODO: In the below piece of code, replace \"None\" with your two input points. You can specify more points if needed, but please make sure to increase the labels as well. \n",
    "\n",
    "input_point = None\n",
    "input_label = np.array([1, 1])\n",
    "\n",
    "mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask from previous iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f96a1",
   "metadata": {
    "id": "d98f96a1"
   },
   "outputs": [],
   "source": [
    "masks, scores, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    mask_input=mask_input[None, :, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8b82f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ce8b82f",
    "outputId": "81d21ac0-5a48-4d0b-9a65-026aa5675a07"
   },
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d5c8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "id": "e06d5c8d",
    "outputId": "8ec4f3d9-8e58-4be2-da9b-c7432bbf76a6"
   },
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example image 2\n",
    "\n",
    "Open the second example image, create an object, and display it with grid for estimating point and box coordinates.   "
   ],
   "id": "a5886f26f5e2dab2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "image = Image.open('data/images/20180917-115018-reduced.jpg')\n",
    "image = np.array(image.convert(\"RGB\"))"
   ],
   "id": "e009714f9b187a66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.grid(visible=True)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ],
   "id": "3192a6722dc80057"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Replace image in the predictor with the new image \n",
    "predictor.set_image(image)"
   ],
   "id": "3b0281945620927c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c93e2087",
   "metadata": {
    "id": "c93e2087"
   },
   "source": [
    "## Activity 2: Specifying an object or region using multiple points (foreground and background) \n",
    "To exclude glacier and just include the glacial discharge, a background point (with label 0) can be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a196f68",
   "metadata": {
    "id": "9a196f68"
   },
   "outputs": [],
   "source": [
    "# E.g., input format for specifying three points\n",
    "# input_point = np.array([[x1, y1], [x2, y2], [x3, y3]])\n",
    "# input_label = np.array([1, 1, 0])\n",
    "\n",
    "# TODO: In the below piece of code, use two or three input points, of which at least one needs to be a background point (water). You can specify more points if needed, but please make sure to increase the labels as well.\n",
    "input_point = None\n",
    "input_label = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a52282",
   "metadata": {
    "id": "81a52282"
   },
   "outputs": [],
   "source": [
    "masks, scores, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca709f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "id": "bfca709f",
    "outputId": "364d55e1-7a96-4e3b-99b3-4c76d205e817"
   },
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2d5a9",
   "metadata": {
    "id": "41e2d5a9"
   },
   "source": "## Activity 3: Specifying a specific object with a box"
  },
  {
   "cell_type": "markdown",
   "id": "d61ca7ac",
   "metadata": {
    "id": "d61ca7ac"
   },
   "source": "The model can also take a box as input, provided in (x1, y1, x2, y2) format."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea92a7b",
   "metadata": {
    "id": "8ea92a7b"
   },
   "outputs": [],
   "source": [
    "# E.g., input format for specifying a box\n",
    "# input_box = np.array([x1, y1, x2, y2])\n",
    "\n",
    "# TODO: In the below piece of code, replace \"None\" with a box coordinate.\n",
    "input_box = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a8814",
   "metadata": {
    "id": "b35a8814"
   },
   "outputs": [],
   "source": [
    "masks, scores, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb4906",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "id": "3ffb4906",
    "outputId": "231f68bc-3b54-4ace-e01a-4fe8ae4b2366"
   },
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, box_coords=input_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed9f0a",
   "metadata": {
    "id": "c1ed9f0a"
   },
   "source": "## Combining points and boxes (Optional)"
  },
  {
   "cell_type": "markdown",
   "id": "8455d1c5",
   "metadata": {
    "id": "8455d1c5"
   },
   "source": "Points and boxes may be combined, just by including both types of prompts to the predictor. Here this can be used to select glacial discharge region from the bounding box."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2e547",
   "metadata": {
    "id": "90e2e547"
   },
   "outputs": [],
   "source": [
    "input_box = np.array([400, 150, 640, 400])\n",
    "input_point = np.array([[550, 350], [550, 225]])\n",
    "input_label = np.array([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956d8c4",
   "metadata": {
    "id": "6956d8c4"
   },
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    box=input_box,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb519a31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "id": "eb519a31",
    "outputId": "f4081bee-d476-4cdb-9879-05f1e1276087"
   },
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, box_coords=input_box, point_coords=input_point, input_labels=input_label)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Batched prompt inputs (Optional)",
   "id": "45ddbca3"
  },
  {
   "cell_type": "markdown",
   "id": "df6f18a0",
   "metadata": {
    "id": "df6f18a0"
   },
   "source": [
    "`SAM2ImagePredictor` can take multiple input prompts for the same image, using `predict` method. For example, imagine we have several box outputs from an object detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06681b",
   "metadata": {
    "id": "0a06681b"
   },
   "outputs": [],
   "source": [
    "input_boxes = np.array([\n",
    "    [400, 100, 640, 400],\n",
    "    [0, 0, 640, 350]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117521a3",
   "metadata": {
    "id": "117521a3"
   },
   "outputs": [],
   "source": [
    "masks, scores, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_boxes,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f5d49",
   "metadata": {
    "id": "6a8f5d49",
    "outputId": "1894cf9f-2be1-4d59-bb47-f17a17c1d1d1"
   },
   "outputs": [],
   "source": [
    "masks.shape  # (batch_size) x (num_predicted_masks_per_input) x H x W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c3681",
   "metadata": {
    "id": "c00c3681",
    "outputId": "65740830-240b-4da1-ffc3-85f821856b9f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "for mask in masks:\n",
    "    show_mask(mask.squeeze(0), plt.gca(), random_color=True)\n",
    "for box in input_boxes:\n",
    "    show_box(box, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Automatic MAsk Generation (Optional)",
   "id": "ba6b11a1ba6dc95b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "def show_anns(anns, borders=True):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "        img[m] = color_mask\n",
    "        if borders:\n",
    "            import cv2\n",
    "            contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            # Try to smooth contours\n",
    "            contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "            cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1)\n",
    "\n",
    "    ax.imshow(img)"
   ],
   "id": "aed05ce9df4d0c14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mask_generator = SAM2AutomaticMaskGenerator(sam2_model)\n",
    "masks = mask_generator.generate(image)"
   ],
   "id": "4ec451b7a6883f02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(len(masks))\n",
    "print(masks[0].keys())"
   ],
   "id": "8ddb887dc0488fc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "2a13c0351f86d382"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
