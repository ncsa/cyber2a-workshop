{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Copyright (c) Meta Platforms, Inc. and affiliates.",
   "id": "57321a1887884586"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cyber2A Segmentation Hands-on Session Using Segment Anything Model 2 (SAM 2) ",
   "id": "8f20cc4914937e6a"
  },
  {
   "cell_type": "markdown",
   "id": "b4a4b25c",
   "metadata": {
    "id": "b4a4b25c"
   },
   "source": [
    "Disclaimer: This notebook reuses some code segments (e.g., helper methods, imports, loading the model, etc.) originally published in the Segment Anything Model 2 (SAM 2) repository (https://github.com/facebookresearch/sam2/blob/main/notebooks/image_predictor_example.ipynb). We have modified the examples to use data files from Arctic datasets and included specific activities for the Cyber2A Workshop.\n",
    "\n",
    "SAM-2 is a Promptable Visual Segmentation (PVS) model trained on large-scale generic data that can predict object segmentation masks based on input prompts. These prompts can be a point, bounding box (i.e., a rectangle), mask, or a combination.\n",
    "\n",
    "The model converts the image into an image embedding (a dense vector representation of the image), which is used to predict masks based on a user prompt.\n",
    "\n",
    "The `SAM2ImagePredictor` class provides an easy interface to the model. Users can attach the input image to the model using its `set_image` method, which calculates the image embeddings. Then, users can use the `predict` method to share prompts (user inputs) that help with the segmentation mask prediction."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set-up",
   "id": "0be845da"
  },
  {
   "cell_type": "markdown",
   "id": "33681dd1",
   "metadata": {
    "id": "33681dd1"
   },
   "source": "Necessary package and checkpoints' imports and helper functions for displaying points, boxes, and masks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b28288",
   "metadata": {
    "id": "69b28288"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741223218cc54186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SAM-2 checkpoints (saved versions of the model along with its parameters).\n",
    "os.chdir(\"SAM_checkpoints\")\n",
    "!sh download_checkpoints.sh\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a15e2f-c7e1-4e5d-862f-fcb751a60b89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33a15e2f-c7e1-4e5d-862f-fcb751a60b89",
    "outputId": "b2779686-e178-4076-8c8c-aed237e51b64"
   },
   "outputs": [],
   "source": [
    "# Select the device for computation. We will be using CUDA to run this notebook. Other options are provided for running this notebook in different environments.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc90d5",
   "metadata": {
    "id": "29bc90d5"
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23842fb2",
   "metadata": {
    "id": "23842fb2"
   },
   "source": [
    "## Example image - 1\n",
    "\n",
    "Open the first example image, create an object, and display it with grid for estimating point and box coordinates.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e4f6b",
   "metadata": {
    "id": "3c2e4f6b"
   },
   "outputs": [],
   "source": [
    "image = Image.open('data/images/20180917-112527.jpg')\n",
    "image = np.array(image.convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30125fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "e30125fd",
    "outputId": "ebc2bf0c-9839-446b-8f84-56d19e5fa66e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.grid(visible=True)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b228b8",
   "metadata": {
    "id": "98b228b8"
   },
   "source": "## Loading the SAM 2 model and configuration"
  },
  {
   "cell_type": "markdown",
   "id": "0bb1927b",
   "metadata": {
    "id": "0bb1927b"
   },
   "source": "Load the SAM 2 model and predictor. Here, we provide the path to a SAM 2 checkpoint, and it's corresponding configuration YAML file (added during SAM 2 installation)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28150b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e28150b",
    "outputId": "0d9f0408-f222-49a5-dee0-455a8feb9419"
   },
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "sam2_checkpoint = \"SAM_checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c925e829",
   "metadata": {
    "id": "c925e829"
   },
   "source": "Process the image to produce an image embedding by calling `SAM2ImagePredictor.set_image`. `SAM2ImagePredictor` stores this embedding and will use it for subsequent mask prediction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d48dd",
   "metadata": {
    "id": "d95d48dd"
   },
   "outputs": [],
   "source": [
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc7a46",
   "metadata": {
    "id": "d8fc7a46"
   },
   "source": [
    "### Specifying an object or region using a single point\n",
    "\n",
    "In this example image, to prompt for the glacier region, let's choose a point on it. \n",
    "\n",
    "Points are a type of input to the model. It's represented in (x,y) format and comes with corresponding labels 1 or 0, which are used to represent foreground and background respectively. As we will see later, we can use multiple points as input, but here we use only one. The show_points method displays the selected point using a star icon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69570c",
   "metadata": {
    "id": "5c69570c"
   },
   "outputs": [],
   "source": [
    "input_point = np.array([[5000, 1000]])\n",
    "input_label = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ba973",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "a91ba973",
    "outputId": "539ccb56-0f84-438c-b34d-658b643673af"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073a838",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1073a838",
    "outputId": "be3c2f7d-281c-44fb-a18b-5d8bc7c612bf"
   },
   "outputs": [],
   "source": [
    "# Display the image embedding feature dimension\n",
    "print(predictor._features[\"image_embed\"].shape, predictor._features[\"image_embed\"][-1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765e952",
   "metadata": {
    "id": "c765e952"
   },
   "source": "Predict segmentation mask with `SAM2ImagePredictor.predict`. The model returns segmentation masks, quality predictions for those masks, and low resolution mask logits that can be passed to the next iteration of prediction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373fd68",
   "metadata": {
    "id": "5373fd68"
   },
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True,\n",
    ")\n",
    "sorted_ind = np.argsort(scores)[::-1] # Sorting the scores in decreasing order\n",
    "masks = masks[sorted_ind]\n",
    "scores = scores[sorted_ind]\n",
    "logits = logits[sorted_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0e938",
   "metadata": {
    "id": "c7f0e938"
   },
   "source": [
    "With `multimask_output=True` (the default setting), SAM 2 outputs 3 masks, where `scores` gives the model's own estimation of the quality of these masks. This setting is intended for ambiguous input prompts, and helps the model disambiguate different objects consistent with the prompt. When `False`, it will return a single mask. For ambiguous prompts such as a single point, it is recommended to use `multimask_output=True` even if only a single mask is desired; the best single mask can be chosen by picking the one with the highest score returned in `scores`. This will often result in a better mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47821187",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47821187",
    "outputId": "8e203298-c55a-4a0a-cfb1-13a04f61eae7"
   },
   "outputs": [],
   "source": [
    "masks.shape  # (number_of_masks) x H x W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c227a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e9c227a6",
    "outputId": "128ad5c6-35fa-4bd3-a3b5-9489677eaf88"
   },
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label, borders=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa31f7c",
   "metadata": {
    "id": "3fa31f7c"
   },
   "source": "## Activity - 1 - Specifying an object or region using multiple points"
  },
  {
   "cell_type": "markdown",
   "id": "88d6d29a",
   "metadata": {
    "id": "88d6d29a"
   },
   "source": "We can see that the single input point can be ambiguous, and the model has returned multiple sub-regions within the glacier image. To obtain a single object or region without ambiguity, multiple points can be provided. If available, a mask from a previous iteration can also be supplied to the model to aid in prediction. When specifying a single object with multiple prompts, a single mask can be requested by setting `multimask_output=False`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6923b94",
   "metadata": {
    "id": "f6923b94"
   },
   "outputs": [],
   "source": [
    "# E.g., input format for specifying two points\n",
    "# input_point = np.array([[x1, y1], [x2, y2]])\n",
    "# input_label = np.array([1, 1])\n",
    "\n",
    "# TODO: In the below piece of code, replace \"None\" with your two input points. You can specify more points if needed, but please make sure to increase the labels as well. \n",
    "\n",
    "input_point = None\n",
    "input_label = np.array([1, 1])\n",
    "\n",
    "mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f96a1",
   "metadata": {
    "id": "d98f96a1"
   },
   "outputs": [],
   "source": [
    "masks, scores, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    mask_input=mask_input[None, :, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8b82f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ce8b82f",
    "outputId": "81d21ac0-5a48-4d0b-9a65-026aa5675a07"
   },
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d5c8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "id": "e06d5c8d",
    "outputId": "8ec4f3d9-8e58-4be2-da9b-c7432bbf76a6"
   },
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e2087",
   "metadata": {
    "id": "c93e2087"
   },
   "source": [
    "To exclude the car and specify just the window, a background point (with label 0, here shown in red) can be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a196f68",
   "metadata": {
    "id": "9a196f68"
   },
   "outputs": [],
   "source": [
    "input_point = np.array([[3500, 375], [500, 375]])\n",
    "input_label = np.array([1, 0])\n",
    "\n",
    "mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a52282",
   "metadata": {
    "id": "81a52282"
   },
   "outputs": [],
   "source": [
    "masks, scores, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    mask_input=mask_input[None, :, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca709f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "id": "bfca709f",
    "outputId": "364d55e1-7a96-4e3b-99b3-4c76d205e817"
   },
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2d5a9",
   "metadata": {
    "id": "41e2d5a9"
   },
   "source": [
    "## Specifying a specific object with a box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ca7ac",
   "metadata": {
    "id": "d61ca7ac"
   },
   "source": "The model can also take a box as input, provided in (x1, y1, x2, y2) format."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea92a7b",
   "metadata": {
    "id": "8ea92a7b"
   },
   "outputs": [],
   "source": [
    "input_box = np.array([3000, 0, 5400, 3600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a8814",
   "metadata": {
    "id": "b35a8814"
   },
   "outputs": [],
   "source": [
    "masks, scores, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb4906",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "id": "3ffb4906",
    "outputId": "231f68bc-3b54-4ace-e01a-4fe8ae4b2366"
   },
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, box_coords=input_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed9f0a",
   "metadata": {
    "id": "c1ed9f0a"
   },
   "source": [
    "## Combining points and boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455d1c5",
   "metadata": {
    "id": "8455d1c5"
   },
   "source": [
    "Points and boxes may be combined, just by including both types of prompts to the predictor. Here this can be used to select just the trucks's tire, instead of the entire wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2e547",
   "metadata": {
    "id": "90e2e547"
   },
   "outputs": [],
   "source": [
    "input_box = np.array([3000, 0, 5400, 3600])\n",
    "input_point = np.array([[3500, 3500], [5000, 3500]])\n",
    "input_label = np.array([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956d8c4",
   "metadata": {
    "id": "6956d8c4"
   },
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    box=input_box,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb519a31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "id": "eb519a31",
    "outputId": "f4081bee-d476-4cdb-9879-05f1e1276087"
   },
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, box_coords=input_box, point_coords=input_point, input_labels=input_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ddbca3",
   "metadata": {
    "id": "45ddbca3"
   },
   "source": [
    "## Batched prompt inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f18a0",
   "metadata": {
    "id": "df6f18a0"
   },
   "source": [
    "`SAM2ImagePredictor` can take multiple input prompts for the same image, using `predict` method. For example, imagine we have several box outputs from an object detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06681b",
   "metadata": {
    "id": "0a06681b"
   },
   "outputs": [],
   "source": [
    "input_boxes = np.array([\n",
    "    [75, 275, 1725, 850],\n",
    "    [425, 600, 700, 875],\n",
    "    [1375, 550, 1650, 800],\n",
    "    [1240, 675, 1400, 750],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117521a3",
   "metadata": {
    "id": "117521a3"
   },
   "outputs": [],
   "source": [
    "masks, scores, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_boxes,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f5d49",
   "metadata": {
    "id": "6a8f5d49",
    "outputId": "1894cf9f-2be1-4d59-bb47-f17a17c1d1d1"
   },
   "outputs": [],
   "source": [
    "masks.shape  # (batch_size) x (num_predicted_masks_per_input) x H x W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c3681",
   "metadata": {
    "id": "c00c3681",
    "outputId": "65740830-240b-4da1-ffc3-85f821856b9f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "for mask in masks:\n",
    "    show_mask(mask.squeeze(0), plt.gca(), random_color=True)\n",
    "for box in input_boxes:\n",
    "    show_box(box, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a27b5d",
   "metadata": {
    "id": "b9a27b5d"
   },
   "source": [
    "## End-to-end batched inference\n",
    "If all prompts are available in advance, it is possible to run SAM 2 directly in an end-to-end fashion. This also allows batching over images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485f75b",
   "metadata": {
    "id": "d485f75b"
   },
   "outputs": [],
   "source": [
    "image1 = image  # truck.jpg from above\n",
    "image1_boxes = np.array([\n",
    "    [75, 275, 1725, 850],\n",
    "    [425, 600, 700, 875],\n",
    "    [1375, 550, 1650, 800],\n",
    "    [1240, 675, 1400, 750],\n",
    "])\n",
    "\n",
    "image2 = Image.open('images/groceries.jpg')\n",
    "image2 = np.array(image2.convert(\"RGB\"))\n",
    "image2_boxes = np.array([\n",
    "    [450, 170, 520, 350],\n",
    "    [350, 190, 450, 350],\n",
    "    [500, 170, 580, 350],\n",
    "    [580, 170, 640, 350],\n",
    "])\n",
    "\n",
    "img_batch = [image1, image2]\n",
    "boxes_batch = [image1_boxes, image2_boxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47932c99",
   "metadata": {
    "id": "47932c99"
   },
   "outputs": [],
   "source": [
    "predictor.set_image_batch(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af3c54",
   "metadata": {
    "id": "97af3c54"
   },
   "outputs": [],
   "source": [
    "masks_batch, scores_batch, _ = predictor.predict_batch(\n",
    "    None,\n",
    "    None,\n",
    "    box_batch=boxes_batch,\n",
    "    multimask_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226df881",
   "metadata": {
    "id": "226df881",
    "outputId": "b3518598-cc2c-4ef5-f8f5-5e3055a9d6e8"
   },
   "outputs": [],
   "source": [
    "for image, boxes, masks in zip(img_batch, boxes_batch, masks_batch):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    for mask in masks:\n",
    "        show_mask(mask.squeeze(0), plt.gca(), random_color=True)\n",
    "    for box in boxes:\n",
    "        show_box(box, plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f30085",
   "metadata": {
    "id": "46f30085"
   },
   "source": [
    "Similarly, we can have a batch of point prompts defined over a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab929fc",
   "metadata": {
    "id": "1ab929fc"
   },
   "outputs": [],
   "source": [
    "image1 = image  # truck.jpg from above\n",
    "image1_pts = np.array([\n",
    "    [[500, 375]],\n",
    "    [[650, 750]]\n",
    "    ]) # Bx1x2 where B corresponds to number of objects\n",
    "image1_labels = np.array([[1], [1]])\n",
    "\n",
    "image2_pts = np.array([\n",
    "    [[400, 300]],\n",
    "    [[630, 300]],\n",
    "])\n",
    "image2_labels = np.array([[1], [1]])\n",
    "\n",
    "pts_batch = [image1_pts, image2_pts]\n",
    "labels_batch = [image1_labels, image2_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f8287",
   "metadata": {
    "id": "848f8287"
   },
   "outputs": [],
   "source": [
    "masks_batch, scores_batch, _ = predictor.predict_batch(pts_batch, labels_batch, box_batch=None, multimask_output=True)\n",
    "\n",
    "# Select the best single mask per object\n",
    "best_masks = []\n",
    "for masks, scores in zip(masks_batch,scores_batch):\n",
    "    best_masks.append(masks[range(len(masks)), np.argmax(scores, axis=-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b15c6c",
   "metadata": {
    "id": "99b15c6c",
    "outputId": "22b717a1-e3b4-4d45-f3d2-51a1108d9ba7"
   },
   "outputs": [],
   "source": [
    "for image, points, labels, masks in zip(img_batch, pts_batch, labels_batch, best_masks):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    for mask in masks:\n",
    "        show_mask(mask, plt.gca(), random_color=True)\n",
    "    show_points(points, labels, plt.gca())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
